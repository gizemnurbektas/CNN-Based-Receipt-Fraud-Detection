{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNx9TZqb2RbfJeEkdpOUyh6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gizemnurbektas/CNN-Based-Receipt-Fraud-Detection/blob/main/CNN_Based_Receipt_Fraud_Detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GOgzQQMucns2",
        "outputId": "1d58729c-ff98-4f5f-d2ac-e7a2fcf177e6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/receipt_fraud_cnn\n"
          ]
        }
      ],
      "source": [
        "!mkdir -p /content/receipt_fraud_cnn\n",
        "%cd /content/receipt_fraud_cnn\n",
        "!mkdir -p data\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -q /content/receiptimages.zip -d /content/receipt_fraud_cnn/data\n",
        "!find /content/receipt_fraud_cnn/data -maxdepth 3 -type f | head\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UijfjnuFc-Ko",
        "outputId": "86a7b049-12c6-4b62-8bcc-f1f13968dfa7"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/receipt_fraud_cnn/data/real/072.jpg\n",
            "/content/receipt_fraud_cnn/data/real/058.jpg\n",
            "/content/receipt_fraud_cnn/data/real/068.jpg\n",
            "/content/receipt_fraud_cnn/data/real/061.jpg\n",
            "/content/receipt_fraud_cnn/data/real/064.jpg\n",
            "/content/receipt_fraud_cnn/data/real/112.jpg\n",
            "/content/receipt_fraud_cnn/data/real/084.jpg\n",
            "/content/receipt_fraud_cnn/data/real/063.jpg\n",
            "/content/receipt_fraud_cnn/data/real/067.jpg\n",
            "/content/receipt_fraud_cnn/data/real/086.jpg\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /content\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_-r3daA_c-IL",
        "outputId": "41dce8fe-3da9-43ba-a7df-6400a40bc868"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "receipt_fraud_cnn  receiptimages.zip  sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install gradio opencv-python\n"
      ],
      "metadata": {
        "id": "woaXHRXUc-Fl"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class ReceiptFraudCNN(nn.Module):\n",
        "    def __init__(self, num_classes=2):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 16, 3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(16, 32, 3, padding=1)\n",
        "        self.conv3 = nn.Conv2d(32, 64, 3, padding=1)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.drop = nn.Dropout(0.3)\n",
        "\n",
        "        # input: (3,224,224) -> 3 kez pool -> (64,28,28)\n",
        "        self.fc1 = nn.Linear(64 * 28 * 28, 128)\n",
        "        self.fc2 = nn.Linear(128, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = self.pool(F.relu(self.conv3(x)))\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.drop(F.relu(self.fc1(x)))\n",
        "        return self.fc2(x)\n"
      ],
      "metadata": {
        "id": "1XUqAMoqc-DB"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class ReceiptFraudCNN(nn.Module):\n",
        "    def __init__(self, num_classes=2):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 16, 3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(16, 32, 3, padding=1)\n",
        "        self.conv3 = nn.Conv2d(32, 64, 3, padding=1)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.drop = nn.Dropout(0.3)\n",
        "\n",
        "        # input: (3,224,224) -> 3 kez pool -> (64,28,28)\n",
        "        self.fc1 = nn.Linear(64 * 28 * 28, 128)\n",
        "        self.fc2 = nn.Linear(128, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = self.pool(F.relu(self.conv3(x)))\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.drop(F.relu(self.fc1(x)))\n",
        "        return self.fc2(x)\n"
      ],
      "metadata": {
        "id": "xWmwugjDc-A0"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!find /content/receipt_fraud_cnn/data -maxdepth 4 -type d\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xBXHajoVeNi0",
        "outputId": "fa606d62-ba1d-4789-8e7d-dff832481571"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/receipt_fraud_cnn/data\n",
            "/content/receipt_fraud_cnn/data/real\n",
            "/content/receipt_fraud_cnn/data/receiptimages\n",
            "/content/receipt_fraud_cnn/data/fake\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!find /content/receipt_fraud_cnn/data -maxdepth 5 -type f | head -n 20\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2IXn-dVLeQfv",
        "outputId": "b1c14e0d-aa7b-4e5a-d05e-e2c1fed72192"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/receipt_fraud_cnn/data/real/072.jpg\n",
            "/content/receipt_fraud_cnn/data/real/058.jpg\n",
            "/content/receipt_fraud_cnn/data/real/068.jpg\n",
            "/content/receipt_fraud_cnn/data/real/061.jpg\n",
            "/content/receipt_fraud_cnn/data/real/064.jpg\n",
            "/content/receipt_fraud_cnn/data/real/112.jpg\n",
            "/content/receipt_fraud_cnn/data/real/084.jpg\n",
            "/content/receipt_fraud_cnn/data/real/063.jpg\n",
            "/content/receipt_fraud_cnn/data/real/067.jpg\n",
            "/content/receipt_fraud_cnn/data/real/086.jpg\n",
            "/content/receipt_fraud_cnn/data/real/109.jpg\n",
            "/content/receipt_fraud_cnn/data/real/103.jpg\n",
            "/content/receipt_fraud_cnn/data/real/078.jpg\n",
            "/content/receipt_fraud_cnn/data/real/073.jpg\n",
            "/content/receipt_fraud_cnn/data/real/098.jpg\n",
            "/content/receipt_fraud_cnn/data/real/107.jpg\n",
            "/content/receipt_fraud_cnn/data/real/091.jpg\n",
            "/content/receipt_fraud_cnn/data/real/085.jpg\n",
            "/content/receipt_fraud_cnn/data/real/113.jpg\n",
            "/content/receipt_fraud_cnn/data/real/106.jpg\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import os\n",
        "import glob\n",
        "import random\n",
        "import numpy as np\n",
        "import cv2\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from model import ReceiptFraudCNN\n",
        "\n",
        "IMG_SIZE = 224\n",
        "BATCH = 32\n",
        "EPOCHS = 8\n",
        "LR = 1e-3\n",
        "SEED = 42\n",
        "\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "\n",
        "def list_images(folder):\n",
        "    exts = [\"*.jpg\", \"*.jpeg\", \"*.png\", \"*.JPG\", \"*.JPEG\", \"*.PNG\"]\n",
        "    files = []\n",
        "    for e in exts:\n",
        "        files.extend(glob.glob(os.path.join(folder, e)))\n",
        "    return files\n",
        "\n",
        "def find_real_fake_dirs(root=\"/content/receipt_fraud_cnn/data\"):\n",
        "    real_dir, fake_dir = None, None\n",
        "    for dirpath, dirnames, _ in os.walk(root):\n",
        "        base = os.path.basename(dirpath).lower()\n",
        "        if base == \"real\":\n",
        "            real_dir = dirpath\n",
        "        if base == \"fake\":\n",
        "            fake_dir = dirpath\n",
        "    return real_dir, fake_dir\n",
        "\n",
        "def read_rgb(path):\n",
        "    img = cv2.imread(path)\n",
        "    if img is None:\n",
        "        raise ValueError(f\"Cannot read: {path}\")\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "    return img\n",
        "\n",
        "def augment(img):\n",
        "    if random.random() < 0.5:\n",
        "        angle = random.uniform(-6, 6)\n",
        "        h, w = img.shape[:2]\n",
        "        M = cv2.getRotationMatrix2D((w/2, h/2), angle, 1.0)\n",
        "        img = cv2.warpAffine(img, M, (w, h), borderMode=cv2.BORDER_REPLICATE)\n",
        "\n",
        "    if random.random() < 0.5:\n",
        "        alpha = random.uniform(0.85, 1.15)\n",
        "        beta  = random.uniform(-15, 15)\n",
        "        img = cv2.convertScaleAbs(img, alpha=alpha, beta=beta)\n",
        "\n",
        "    if random.random() < 0.3:\n",
        "        img = cv2.GaussianBlur(img, (3,3), 0)\n",
        "\n",
        "    return img\n",
        "\n",
        "def preprocess(img):\n",
        "    img = cv2.resize(img, (IMG_SIZE, IMG_SIZE), interpolation=cv2.INTER_AREA)\n",
        "    img = img.astype(np.float32) / 255.0\n",
        "    img = np.transpose(img, (2, 0, 1))\n",
        "    return img\n",
        "\n",
        "class RealFakeDataset(Dataset):\n",
        "    def __init__(self, real_paths, fake_paths):\n",
        "        self.items = [(p, 1) for p in real_paths] + [(p, 0) for p in fake_paths]\n",
        "        random.shuffle(self.items)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.items)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        path, label = self.items[idx]\n",
        "        img = read_rgb(path)\n",
        "        img = augment(img)\n",
        "        x = preprocess(img)\n",
        "        return torch.tensor(x, dtype=torch.float32), torch.tensor(label, dtype=torch.long)\n",
        "\n",
        "def split_items(items, val_ratio=0.2):\n",
        "    n = len(items)\n",
        "    n_val = int(n * val_ratio)\n",
        "    return items[n_val:], items[:n_val]\n",
        "\n",
        "def main():\n",
        "    real_dir, fake_dir = find_real_fake_dirs()\n",
        "    print(\"Found real_dir:\", real_dir)\n",
        "    print(\"Found fake_dir:\", fake_dir)\n",
        "\n",
        "    if real_dir is None or fake_dir is None:\n",
        "        raise RuntimeError(\"real/fake klasörleri bulunamadı. unzip sonrası yolu kontrol et.\")\n",
        "\n",
        "    real_paths = list_images(real_dir)\n",
        "    fake_paths = list_images(fake_dir)\n",
        "\n",
        "    print(\"real files:\", len(real_paths))\n",
        "    print(\"fake files:\", len(fake_paths))\n",
        "\n",
        "    if len(real_paths) < 10 or len(fake_paths) < 10:\n",
        "        raise RuntimeError(f\"Yetersiz veri. real={len(real_paths)}, fake={len(fake_paths)}\")\n",
        "\n",
        "    # Dengele\n",
        "    k = min(len(real_paths), len(fake_paths))\n",
        "    real_paths = random.sample(real_paths, k)\n",
        "    fake_paths = random.sample(fake_paths, k)\n",
        "\n",
        "    dataset = [(p, 1) for p in real_paths] + [(p, 0) for p in fake_paths]\n",
        "    random.shuffle(dataset)\n",
        "\n",
        "    train_items, val_items = split_items(dataset, val_ratio=0.2)\n",
        "\n",
        "    train_real = [p for p,l in train_items if l==1]\n",
        "    train_fake = [p for p,l in train_items if l==0]\n",
        "    val_real   = [p for p,l in val_items if l==1]\n",
        "    val_fake   = [p for p,l in val_items if l==0]\n",
        "\n",
        "    train_ds = RealFakeDataset(train_real, train_fake)\n",
        "    val_ds   = RealFakeDataset(val_real, val_fake)\n",
        "\n",
        "    train_loader = DataLoader(train_ds, batch_size=BATCH, shuffle=True)\n",
        "    val_loader   = DataLoader(val_ds, batch_size=BATCH, shuffle=False)\n",
        "\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    model = ReceiptFraudCNN().to(device)\n",
        "    opt = torch.optim.Adam(model.parameters(), lr=LR)\n",
        "    loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "    best_acc = 0.0\n",
        "\n",
        "    for epoch in range(EPOCHS):\n",
        "        model.train()\n",
        "        total_loss = 0.0\n",
        "        for x, y in train_loader:\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            opt.zero_grad()\n",
        "            logits = model(x)\n",
        "            loss = loss_fn(logits, y)\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        model.eval()\n",
        "        correct, total = 0, 0\n",
        "        with torch.no_grad():\n",
        "            for x, y in val_loader:\n",
        "                x, y = x.to(device), y.to(device)\n",
        "                pred = model(x).argmax(dim=1)\n",
        "                correct += (pred == y).sum().item()\n",
        "                total += y.size(0)\n",
        "\n",
        "        acc = correct / max(total, 1)\n",
        "        print(f\"Epoch {epoch+1}/{EPOCHS} | loss={total_loss/len(train_loader):.4f} | val_acc={acc:.3f}\")\n",
        "\n",
        "        if acc > best_acc:\n",
        "            best_acc = acc\n",
        "            torch.save(model.state_dict(), \"/content/receipt_fraud_cnn/cnn_model.pth\")\n",
        "\n",
        "    print(\"Best val acc:\", best_acc)\n",
        "    print(\"Saved: /content/receipt_fraud_cnn/cnn_model.pth\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yoJRu5d_c9-P",
        "outputId": "4efe91fe-bb4c-4b76-a9b4-d6c2144daac6"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found real_dir: /content/receipt_fraud_cnn/data/real\n",
            "Found fake_dir: /content/receipt_fraud_cnn/data/fake\n",
            "real files: 62\n",
            "fake files: 52\n",
            "Epoch 1/8 | loss=0.7753 | val_acc=0.500\n",
            "Epoch 2/8 | loss=0.7144 | val_acc=0.500\n",
            "Epoch 3/8 | loss=0.6932 | val_acc=0.500\n",
            "Epoch 4/8 | loss=0.6873 | val_acc=0.650\n",
            "Epoch 5/8 | loss=0.6736 | val_acc=0.550\n",
            "Epoch 6/8 | loss=0.6461 | val_acc=0.700\n",
            "Epoch 7/8 | loss=0.5933 | val_acc=0.600\n",
            "Epoch 8/8 | loss=0.5596 | val_acc=0.550\n",
            "Best val acc: 0.7\n",
            "Saved: /content/receipt_fraud_cnn/cnn_model.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/receipt_fraud_cnn/train.py\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pxPziNUhc93T",
        "outputId": "d757c8a1-6df8-4561-d2ae-7e892105c5b7"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found real_dir: /content/receipt_fraud_cnn/data/real\n",
            "Found fake_dir: /content/receipt_fraud_cnn/data/fake\n",
            "real files: 62\n",
            "fake files: 52\n",
            "Epoch 1/8 | loss=0.7753 | val_acc=0.500\n",
            "Epoch 2/8 | loss=0.7144 | val_acc=0.500\n",
            "Epoch 3/8 | loss=0.6932 | val_acc=0.500\n",
            "Epoch 4/8 | loss=0.6873 | val_acc=0.650\n",
            "Epoch 5/8 | loss=0.6736 | val_acc=0.550\n",
            "Epoch 6/8 | loss=0.6461 | val_acc=0.700\n",
            "Epoch 7/8 | loss=0.5933 | val_acc=0.600\n",
            "Epoch 8/8 | loss=0.5596 | val_acc=0.550\n",
            "Best val acc: 0.7\n",
            "Saved: /content/receipt_fraud_cnn/cnn_model.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import gradio as gr\n",
        "from PIL import Image\n",
        "import cv2\n",
        "\n",
        "from model import ReceiptFraudCNN\n",
        "\n",
        "IMG_SIZE = 224\n",
        "MODEL_PATH = \"/content/receipt_fraud_cnn/cnn_model.pth\"\n",
        "LABELS = {1: \"REAL (Orijinal)\", 0: \"FAKE (Manipülasyon Şüphesi)\"}\n",
        "\n",
        "def preprocess_pil(pil_img):\n",
        "    img = np.array(pil_img.convert(\"RGB\"))\n",
        "    img = cv2.resize(img, (IMG_SIZE, IMG_SIZE), interpolation=cv2.INTER_AREA)\n",
        "    img = img.astype(np.float32) / 255.0\n",
        "    img = np.transpose(img, (2, 0, 1))\n",
        "    x = torch.tensor(img, dtype=torch.float32).unsqueeze(0)\n",
        "    return x\n",
        "\n",
        "if not os.path.exists(MODEL_PATH):\n",
        "    raise FileNotFoundError(f\"Model yok: {MODEL_PATH}. Önce train.py çalıştır.\")\n",
        "\n",
        "model = ReceiptFraudCNN()\n",
        "model.load_state_dict(torch.load(MODEL_PATH, map_location=\"cpu\"))\n",
        "model.eval()\n",
        "softmax = torch.nn.Softmax(dim=1)\n",
        "\n",
        "def predict(image):\n",
        "    if image is None:\n",
        "        return \"Error: Görsel yükleyin.\", 0.0\n",
        "\n",
        "    x = preprocess_pil(image)\n",
        "    with torch.no_grad():\n",
        "        logits = model(x)\n",
        "        probs = softmax(logits).numpy()[0]\n",
        "        pred = int(np.argmax(probs))\n",
        "        score = float(probs[pred])\n",
        "\n",
        "    return LABELS[pred], score\n",
        "\n",
        "demo = gr.Interface(\n",
        "    fn=predict,\n",
        "    inputs=gr.Image(type=\"pil\", label=\"Fiş/Dekont Görseli Yükle\"),\n",
        "    outputs=[gr.Textbox(label=\"Sonuç\"), gr.Number(label=\"Güven Skoru\")],\n",
        "    title=\"Receipt Fraud Detector (CNN)\",\n",
        "    description=\"Real/Fake fiş görselleriyle eğitilmiş basit CNN modeli. Görsel yükle, sonucu al.\",\n",
        ")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    demo.launch(share=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YsjPwM7wc91G",
        "outputId": "13bae4e8-f2c6-49bf-979d-416e1a9d141b"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing /content/receipt_fraud_cnn/serve.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/receipt_fraud_cnn/serve.py\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "62pXWJY6c9yv",
        "outputId": "7f76af29-0f92-4181-80ff-a2be7bd21202"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "* Running on local URL:  http://127.0.0.1:7860\n",
            "* Running on public URL: https://d64622b9ed9c411fb5.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "N6EhV30kc9wa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2uJgR6gZc9t0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0ICYJXtGc9qU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TZf6vWIUc9i7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}